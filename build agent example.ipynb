{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazaioan/anaconda3/envs/RLATorch/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import DatasetUCI\n",
    "from envs import LalEnvFirstAccuracy\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from replay_buffer import ReplayBuffer\n",
    "from dqn import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "1. CHANGE THE CODE ACCORDING TO THE SITE: https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda\n",
    "\n",
    "2. USE LLAVA TO ASK THE QUESTION FROM STACK OVERFLOW AND UPDATE THE ORIGINAL CODE IN TENSORFLOW!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and initialisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WARM_START_EPISODES = 2 # Number of warm-start episodes to create transitions for the Replay Buffer.\n",
    "STATE_LENGTH = 30 # Size of the state data.\n",
    "TRAIN_SET_SIZE = -1 # Use -1 for using all data points.\n",
    "SUBSET = -1 # Use -1 for using all data points, 0 for even, 1 for odd.\n",
    "N_JOBS = 1 # Can set more if we want to parallelise.\n",
    "# Remove the dataset that will be used for testing.\n",
    "# ['australian', 'breast_cancer', 'diabetis', 'flare_solar', 'german', 'heart', 'mushrooms', 'waveform', 'wdbc']\n",
    "# possible_dataset_names = ['breast_cancer', 'diabetis', 'flare_solar', 'german', 'heart', 'mushrooms', 'waveform', 'wdbc']\n",
    "possible_dataset_names = ['waveform']\n",
    "test_dataset_names = ['diabetis']\n",
    "# The quality is measured according to a given quality measure \"quality_method\". \n",
    "QUALITY_METHOD = metrics.accuracy_score\n",
    "BUFFER_SIZE = 10000 # Size of the Replay Buffer.\n",
    "BATCH_SIZE = 32 # Size of the Minibatch size for the Replay Buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = DatasetUCI(possible_dataset_names, n_state_estimation=STATE_LENGTH, subset=SUBSET, size=TRAIN_SET_SIZE)\n",
    "# If we want to measure test error along with training.\n",
    "dataset_test = DatasetUCI(test_dataset_names, n_state_estimation=STATE_LENGTH, subset=SUBSET, size=TRAIN_SET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = LalEnvFirstAccuracy(dataset, model, quality_method=QUALITY_METHOD)\n",
    "env_test = LalEnvFirstAccuracy(dataset_test, model, quality_method=QUALITY_METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the Replay Buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(Buffer_Size=BUFFER_SIZE, Batch_Size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warm - start episodes for the Replay Buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1.\n",
      "\n",
      "\n",
      "n.actions 2463\n",
      "n.actions 2440\n",
      "n.actions 2426\n",
      "Final achieved ACC is 0.7952.\n",
      "Episode 1. | Budget size is 49.\n",
      "Episode 2.\n",
      "\n",
      "\n",
      "n.actions 2463\n",
      "n.actions 2446\n",
      "n.actions 2406\n",
      "n.actions 2380\n",
      "n.actions 2377\n",
      "n.actions 2340\n",
      "n.actions 2325\n",
      "n.actions 2323\n",
      "n.actions 2299\n",
      "n.actions 2259\n",
      "n.actions 2226\n",
      "n.actions 2204\n",
      "n.actions 2177\n",
      "n.actions 2154\n",
      "n.actions 2134\n",
      "n.actions 2122\n",
      "n.actions 2114\n",
      "n.actions 2084\n",
      "n.actions 2076\n",
      "n.actions 2074\n",
      "n.actions 2049\n",
      "n.actions 2014\n",
      "n.actions 2000\n",
      "n.actions 1987\n",
      "n.actions 1953\n",
      "n.actions 1948\n",
      "n.actions 1934\n",
      "n.actions 1932\n",
      "n.actions 1928\n",
      "n.actions 1903\n",
      "n.actions 1889\n",
      "n.actions 1884\n",
      "n.actions 1856\n",
      "n.actions 1823\n",
      "n.actions 1795\n",
      "n.actions 1788\n",
      "n.actions 1782\n",
      "n.actions 1745\n",
      "n.actions 1716\n",
      "n.actions 1683\n",
      "n.actions 1662\n",
      "n.actions 1623\n",
      "n.actions 1590\n",
      "n.actions 1564\n",
      "n.actions 1556\n",
      "n.actions 1538\n",
      "n.actions 1515\n",
      "n.actions 1483\n",
      "n.actions 1457\n",
      "n.actions 1443\n",
      "n.actions 1423\n",
      "n.actions 1394\n",
      "n.actions 1357\n",
      "n.actions 1328\n",
      "n.actions 1297\n",
      "n.actions 1274\n",
      "n.actions 1260\n",
      "n.actions 1257\n",
      "n.actions 1233\n",
      "n.actions 1197\n",
      "n.actions 1173\n",
      "n.actions 1135\n",
      "n.actions 1104\n",
      "n.actions 1070\n",
      "n.actions 1068\n",
      "n.actions 1053\n",
      "n.actions 1021\n",
      "n.actions 998\n",
      "n.actions 983\n",
      "n.actions 970\n",
      "n.actions 936\n",
      "n.actions 922\n",
      "n.actions 900\n",
      "n.actions 860\n",
      "n.actions 853\n",
      "n.actions 831\n",
      "n.actions 819\n",
      "n.actions 785\n",
      "n.actions 763\n",
      "n.actions 739\n",
      "n.actions 710\n",
      "n.actions 689\n",
      "n.actions 657\n",
      "n.actions 647\n",
      "n.actions 623\n",
      "n.actions 585\n",
      "n.actions 582\n",
      "n.actions 566\n",
      "n.actions 547\n",
      "n.actions 516\n",
      "Final achieved ACC is 0.8708.\n",
      "Episode 2. | Budget size is 1980.\n",
      "-507.25\n"
     ]
    }
   ],
   "source": [
    "# Keep track of episode duration to compute average.\n",
    "episode_durations = []\n",
    "episode_number = 1\n",
    "\n",
    "for _ in range(WARM_START_EPISODES):\n",
    "    \n",
    "    print(\"Episode {}.\".format(episode_number))\n",
    "    print(\"\\n\")\n",
    "    # Reset the environment to start a new episode.\n",
    "    # The classifier_state contains vector representation of state of the environment (depends on the classifier).\n",
    "    # The _action contains vector representations of all actions available to be taken at the next step.\n",
    "    state, _action, reward = env.reset()\n",
    "    batch = 6\n",
    "    done = False\n",
    "    episode_duration = 6\n",
    "\n",
    "    # Before we reach a done state, make steps.\n",
    "    while not done:\n",
    "        # Choose a random action.\n",
    "        batch = random.randint(1,40)\n",
    "        if batch > env.n_actions:\n",
    "            done = True\n",
    "            break\n",
    "        # Getting numbers from 0 to n_actions.\n",
    "        print(\"n.actions\", env.n_actions)\n",
    "        inputNumbers =range(0,env.n_actions)\n",
    "        # Non-repeating using sample() function.\n",
    "        a = np.array(random.sample(inputNumbers, batch))\n",
    "        action = _action[:,a]\n",
    "        _state, _action, reward, done = env.step(a)\n",
    "\n",
    "        # Store the transition in the replay buffer.\n",
    "        replay_buffer.store_transition(state, action, reward, _state, _action, done)\n",
    "        # Get ready for next step.\n",
    "        state = _state\n",
    "        episode_duration += batch\n",
    "    print(\"Episode {}. | Budget size is {}.\".format(episode_number, episode_duration))\n",
    "    episode_number += 1\n",
    "    episode_durations.append(episode_duration)\n",
    "\n",
    "# Compute the average episode duration of episodes generated during the warm start procedure.\n",
    "av_episode_duration = np.mean(episode_durations)\n",
    "BIAS_INITIALIZATION = -av_episode_duration/2\n",
    "print(BIAS_INITIALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([Experience(state=array([0.07465001, 0.10851647, 0.14206922, 0.24471618, 0.25059533,\n",
       "              0.29880774, 0.30115846, 0.38222654, 0.47412465, 0.47518037,\n",
       "              0.68852595, 0.69115732, 0.73617578, 0.7600859 , 0.83331704,\n",
       "              0.9097671 , 0.98157631, 0.98723959, 0.9913819 , 0.99756386,\n",
       "              0.99888689, 0.99969446, 0.99981465, 0.99988567, 0.99993366,\n",
       "              0.99994739, 0.99995234, 0.99995889, 0.99998219, 0.99999356]), action=array([[0.916672  , 0.99866922, 0.14422618, 0.99574503, 0.99371512,\n",
       "               0.06563122, 0.28867899, 0.37025928, 0.45938251, 0.9937422 ,\n",
       "               0.99998599, 0.21083801, 0.6498622 , 0.99926106, 0.99829996,\n",
       "               0.09334048, 0.38587836, 0.98309289, 0.99994908, 0.5922601 ,\n",
       "               0.77571492, 0.96807914, 0.99684493, 0.95150298, 0.87072432,\n",
       "               0.10835619, 0.16162364],\n",
       "              [0.98343337, 0.98699572, 0.98710812, 0.9879072 , 0.98931017,\n",
       "               0.99934199, 0.99904492, 0.98609511, 1.00930851, 0.99935129,\n",
       "               0.99457616, 1.00616301, 1.00010526, 1.00545453, 0.99896777,\n",
       "               1.00389952, 0.98941444, 0.99221019, 1.00173975, 0.99605522,\n",
       "               1.01262814, 1.01915806, 0.99663451, 1.00014591, 0.98844935,\n",
       "               0.99517825, 0.98644843],\n",
       "              [0.99331206, 0.99522419, 0.99836078, 0.99802097, 0.99388091,\n",
       "               0.99687025, 1.00634569, 0.99284172, 1.00472517, 1.00590427,\n",
       "               0.99927989, 1.00516373, 0.99128   , 0.99961585, 1.00692414,\n",
       "               1.00070834, 0.99261683, 0.99423329, 1.0062815 , 0.99627718,\n",
       "               1.00714521, 1.00323269, 0.99443126, 1.00252914, 0.99356528,\n",
       "               0.99549725, 0.99893711]]), reward=-0.0020000000000000018, next_state=array([0.07465001, 0.10851647, 0.14206922, 0.24471618, 0.25059533,\n",
       "              0.29880774, 0.30115846, 0.38222654, 0.47412465, 0.47518037,\n",
       "              0.68852595, 0.69115732, 0.73617578, 0.7600859 , 0.83331704,\n",
       "              0.9097671 , 0.98157631, 0.98723959, 0.9913819 , 0.99756386,\n",
       "              0.99888689, 0.99969446, 0.99981465, 0.99988567, 0.99993366,\n",
       "              0.99994739, 0.99995234, 0.99995889, 0.99998219, 0.99999356]), next_action=array([[0.05302901, 0.84245648, 0.11259695, ..., 0.99954112, 0.09254193,\n",
       "               0.02125573],\n",
       "              [1.00765846, 1.00756286, 0.998582  , ..., 0.99720007, 0.98466536,\n",
       "               0.99847493],\n",
       "              [1.00241306, 1.00808577, 1.00278124, ..., 0.99848346, 0.99237162,\n",
       "               0.99457282]]), done=True)],\n",
       "      maxlen=10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "experiences = namedtuple(\"Experience\", field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"next_action\",\"done\"])\n",
    "memory = deque(maxlen=10000)\n",
    "e = experiences(state,action,reward,_state,_action,done)\n",
    "memory.append(e)\n",
    "memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for training RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIRNAME = './agents/1-australian-logreg-8-to-1/' # The resulting agent of this experiment will be written in a file.\n",
    "\n",
    "# Replay buffer parameters.\n",
    "REPLAY_BUFFER_SIZE = 1e4\n",
    "PRIOROTIZED_REPLAY_EXPONENT = 3\n",
    "\n",
    "# Agent parameters.\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_COPY_FACTOR = 0.01\n",
    "BIAS_INITIALIZATION = 0 # Default 0 # will be set to minus half of average duration during warm start experiments.\n",
    "\n",
    "# Warm start parameters.\n",
    "NN_UPDATES_PER_WARM_START = 100\n",
    "\n",
    "# Episode simulation parameters.\n",
    "EPSILON_START = 1\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_STEPS = 1000\n",
    "\n",
    "# Training parameters.\n",
    "TRAINING_ITERATIONS = 10 # Reduce for test.\n",
    "TRAINING_EPISODES_PER_ITERATION = 5 # At each training iteration x episodes are simulated.\n",
    "NN_UPDATES_PER_ITERATION = 5 # At each training iteration x gradient steps are made.\n",
    "\n",
    "# Validation and test parameters.\n",
    "N_VALIDATION = 2 # Reduce for test.\n",
    "N_TEST = 2 # Reduce for test.\n",
    "VALIDATION_TEST_FREQUENCY = 3 # Every x iterations, val and test are performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = DQN(State_Length=STATE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0095, 0.0426, 0.0454, 0.2300, 0.2411, 0.2469, 0.3681, 0.4190, 0.7247,\n",
      "         0.8494, 0.9109, 0.9468, 0.9830, 0.9831, 0.9870, 0.9895, 0.9941, 0.9963,\n",
      "         0.9964, 0.9966, 0.9971, 0.9975, 0.9981, 0.9983, 0.9989, 0.9990, 0.9995,\n",
      "         0.9997, 0.9997, 0.9999],\n",
      "        [0.0654, 0.0925, 0.1642, 0.2194, 0.2484, 0.2761, 0.3502, 0.4157, 0.4752,\n",
      "         0.4830, 0.7113, 0.7163, 0.7369, 0.7710, 0.8164, 0.9192, 0.9830, 0.9897,\n",
      "         0.9913, 0.9980, 0.9990, 0.9997, 0.9998, 0.9999, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0314, 0.1054, 0.1508, 0.1822, 0.2394, 0.2548, 0.2821, 0.3182, 0.3305,\n",
      "         0.4779, 0.5683, 0.7048, 0.7162, 0.7806, 0.8740, 0.8753, 0.8811, 0.9055,\n",
      "         0.9877, 0.9901, 0.9938, 0.9969, 0.9983, 0.9983, 0.9996, 0.9997, 0.9997,\n",
      "         0.9998, 0.9999, 0.9999],\n",
      "        [0.0876, 0.0963, 0.1075, 0.1306, 0.1409, 0.1485, 0.2370, 0.2763, 0.3617,\n",
      "         0.3629, 0.7140, 0.7174, 0.8510, 0.8662, 0.8929, 0.9461, 0.9822, 0.9880,\n",
      "         0.9911, 0.9966, 0.9990, 0.9992, 0.9995, 0.9998, 0.9999, 0.9999, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0502, 0.1027, 0.1329, 0.1460, 0.2420, 0.2905, 0.3281, 0.3644, 0.3951,\n",
      "         0.4554, 0.6456, 0.7451, 0.7481, 0.7792, 0.8062, 0.8947, 0.9759, 0.9787,\n",
      "         0.9856, 0.9976, 0.9985, 0.9995, 0.9997, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0261, 0.0500, 0.0931, 0.1078, 0.2173, 0.2357, 0.2553, 0.3603, 0.6934,\n",
      "         0.9518, 0.9535, 0.9652, 0.9731, 0.9742, 0.9882, 0.9912, 0.9918, 0.9923,\n",
      "         0.9936, 0.9970, 0.9971, 0.9972, 0.9983, 0.9986, 0.9987, 0.9988, 0.9994,\n",
      "         0.9996, 0.9998, 1.0000],\n",
      "        [0.0616, 0.0796, 0.1385, 0.1474, 0.1765, 0.1865, 0.2107, 0.2267, 0.3391,\n",
      "         0.4631, 0.6176, 0.6632, 0.8040, 0.8886, 0.8943, 0.9407, 0.9778, 0.9897,\n",
      "         0.9925, 0.9977, 0.9990, 0.9993, 0.9997, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0433, 0.0865, 0.0906, 0.0926, 0.1509, 0.2434, 0.2684, 0.3333, 0.3551,\n",
      "         0.4039, 0.6787, 0.7775, 0.8264, 0.8325, 0.8533, 0.9287, 0.9831, 0.9904,\n",
      "         0.9917, 0.9979, 0.9994, 0.9997, 0.9998, 0.9999, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0673, 0.1069, 0.1499, 0.1902, 0.2597, 0.2671, 0.2923, 0.3865, 0.4274,\n",
      "         0.4912, 0.6555, 0.7180, 0.7643, 0.7653, 0.7905, 0.9172, 0.9799, 0.9874,\n",
      "         0.9903, 0.9981, 0.9989, 0.9997, 0.9998, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0693, 0.1042, 0.1714, 0.2061, 0.2528, 0.2702, 0.2955, 0.3880, 0.4648,\n",
      "         0.5160, 0.7051, 0.7314, 0.7456, 0.7828, 0.8274, 0.9216, 0.9814, 0.9890,\n",
      "         0.9909, 0.9979, 0.9990, 0.9997, 0.9998, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0692, 0.0969, 0.1649, 0.2189, 0.2501, 0.2809, 0.3343, 0.4076, 0.4821,\n",
      "         0.4869, 0.7001, 0.7126, 0.7292, 0.7612, 0.8198, 0.9131, 0.9818, 0.9888,\n",
      "         0.9911, 0.9978, 0.9989, 0.9996, 0.9998, 0.9999, 0.9999, 0.9999, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0729, 0.1063, 0.1615, 0.1659, 0.2734, 0.2820, 0.3051, 0.3977, 0.4238,\n",
      "         0.4837, 0.6672, 0.7277, 0.7303, 0.7648, 0.7857, 0.9175, 0.9781, 0.9885,\n",
      "         0.9888, 0.9980, 0.9988, 0.9997, 0.9998, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0497, 0.0677, 0.0779, 0.0886, 0.1060, 0.1602, 0.2291, 0.2345, 0.2448,\n",
      "         0.3167, 0.3385, 0.4292, 0.4689, 0.5103, 0.6459, 0.7299, 0.7793, 0.7839,\n",
      "         0.7930, 0.8000, 0.8302, 0.8514, 0.8713, 0.8723, 0.8730, 0.9179, 0.9404,\n",
      "         0.9537, 0.9602, 0.9766],\n",
      "        [0.0666, 0.1111, 0.1753, 0.2105, 0.2373, 0.2633, 0.2841, 0.3859, 0.4577,\n",
      "         0.5188, 0.7072, 0.7305, 0.7656, 0.7825, 0.8355, 0.9177, 0.9805, 0.9883,\n",
      "         0.9901, 0.9976, 0.9990, 0.9997, 0.9998, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0641, 0.0782, 0.1340, 0.1628, 0.1678, 0.1709, 0.1910, 0.2573, 0.3720,\n",
      "         0.4423, 0.6604, 0.6935, 0.8184, 0.8551, 0.8657, 0.9244, 0.9763, 0.9767,\n",
      "         0.9911, 0.9961, 0.9987, 0.9988, 0.9994, 0.9998, 0.9999, 0.9999, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0690, 0.1133, 0.1203, 0.1350, 0.1886, 0.2429, 0.2442, 0.2669, 0.3936,\n",
      "         0.4278, 0.6266, 0.6973, 0.7770, 0.8340, 0.8817, 0.9195, 0.9535, 0.9890,\n",
      "         0.9931, 0.9980, 0.9988, 0.9991, 0.9996, 0.9997, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0518, 0.0862, 0.1377, 0.1412, 0.2022, 0.2801, 0.3316, 0.4065, 0.4290,\n",
      "         0.4659, 0.6619, 0.7141, 0.7416, 0.7664, 0.7925, 0.8813, 0.9769, 0.9840,\n",
      "         0.9843, 0.9977, 0.9984, 0.9995, 0.9997, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0113, 0.0142, 0.0387, 0.1111, 0.1206, 0.1344, 0.1679, 0.1885, 0.3327,\n",
      "         0.4482, 0.4962, 0.5230, 0.5547, 0.5797, 0.5960, 0.7401, 0.7712, 0.8042,\n",
      "         0.9892, 0.9961, 0.9983, 0.9986, 0.9990, 0.9993, 0.9994, 0.9996, 0.9997,\n",
      "         0.9999, 0.9999, 1.0000],\n",
      "        [0.0041, 0.0196, 0.0536, 0.0994, 0.1672, 0.1774, 0.1871, 0.2181, 0.2494,\n",
      "         0.2519, 0.2659, 0.3102, 0.3580, 0.3754, 0.3894, 0.3903, 0.4220, 0.7621,\n",
      "         0.8767, 0.9534, 0.9924, 0.9926, 0.9946, 0.9962, 0.9980, 0.9991, 0.9991,\n",
      "         0.9994, 0.9998, 0.9999],\n",
      "        [0.0711, 0.0896, 0.1053, 0.1236, 0.1426, 0.1906, 0.2420, 0.2934, 0.3690,\n",
      "         0.4103, 0.6802, 0.7198, 0.8043, 0.8268, 0.8637, 0.9517, 0.9874, 0.9877,\n",
      "         0.9936, 0.9975, 0.9994, 0.9994, 0.9996, 0.9998, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0719, 0.1014, 0.1542, 0.2129, 0.2411, 0.2625, 0.2888, 0.3733, 0.4386,\n",
      "         0.5042, 0.7139, 0.7163, 0.7474, 0.7806, 0.8211, 0.9179, 0.9801, 0.9896,\n",
      "         0.9904, 0.9973, 0.9989, 0.9997, 0.9998, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0714, 0.0972, 0.1403, 0.2085, 0.2691, 0.2759, 0.2937, 0.3623, 0.4155,\n",
      "         0.4860, 0.7022, 0.7318, 0.7661, 0.7875, 0.8111, 0.9169, 0.9782, 0.9891,\n",
      "         0.9918, 0.9980, 0.9990, 0.9997, 0.9998, 0.9999, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0706, 0.1053, 0.1554, 0.2421, 0.2778, 0.3241, 0.3320, 0.3931, 0.4785,\n",
      "         0.4804, 0.6904, 0.7033, 0.7069, 0.7516, 0.8161, 0.9126, 0.9826, 0.9882,\n",
      "         0.9910, 0.9974, 0.9988, 0.9996, 0.9998, 0.9999, 0.9999, 0.9999, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0526, 0.0841, 0.1508, 0.1647, 0.2263, 0.2794, 0.3343, 0.4514, 0.4593,\n",
      "         0.4913, 0.7049, 0.7203, 0.7314, 0.7851, 0.8116, 0.9181, 0.9862, 0.9887,\n",
      "         0.9897, 0.9980, 0.9991, 0.9996, 0.9998, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0576, 0.0998, 0.1122, 0.1166, 0.1844, 0.2405, 0.2419, 0.3519, 0.3834,\n",
      "         0.4411, 0.6747, 0.7407, 0.7466, 0.8319, 0.8556, 0.9317, 0.9856, 0.9878,\n",
      "         0.9885, 0.9977, 0.9992, 0.9996, 0.9996, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0769, 0.0920, 0.1161, 0.1260, 0.1451, 0.2008, 0.2407, 0.2854, 0.3651,\n",
      "         0.4107, 0.6603, 0.7074, 0.8013, 0.8276, 0.8561, 0.9483, 0.9868, 0.9871,\n",
      "         0.9926, 0.9972, 0.9993, 0.9993, 0.9995, 0.9998, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0561, 0.0825, 0.1391, 0.1514, 0.2184, 0.3025, 0.3141, 0.4420, 0.4450,\n",
      "         0.4536, 0.6825, 0.7014, 0.7363, 0.7662, 0.7771, 0.8982, 0.9819, 0.9870,\n",
      "         0.9891, 0.9978, 0.9988, 0.9996, 0.9997, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0645, 0.1026, 0.1629, 0.2057, 0.2528, 0.2739, 0.3021, 0.3831, 0.4646,\n",
      "         0.5154, 0.6979, 0.7328, 0.7376, 0.7751, 0.8260, 0.9216, 0.9824, 0.9891,\n",
      "         0.9911, 0.9980, 0.9991, 0.9997, 0.9998, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0044, 0.0159, 0.0325, 0.0666, 0.1238, 0.1556, 0.1722, 0.2821, 0.3359,\n",
      "         0.3645, 0.4546, 0.4572, 0.4948, 0.5471, 0.6404, 0.7706, 0.7907, 0.8228,\n",
      "         0.9778, 0.9936, 0.9987, 0.9990, 0.9993, 0.9994, 0.9995, 0.9997, 0.9997,\n",
      "         0.9999, 0.9999, 1.0000],\n",
      "        [0.0602, 0.0937, 0.0967, 0.0981, 0.1541, 0.2354, 0.2534, 0.3377, 0.3465,\n",
      "         0.4248, 0.6945, 0.7562, 0.7921, 0.8240, 0.8550, 0.9275, 0.9878, 0.9882,\n",
      "         0.9891, 0.9978, 0.9992, 0.9996, 0.9997, 0.9999, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0926, 0.1113, 0.1635, 0.1861, 0.2010, 0.2142, 0.2495, 0.2952, 0.3279,\n",
      "         0.3882, 0.6265, 0.6299, 0.7152, 0.8484, 0.8640, 0.9013, 0.9475, 0.9884,\n",
      "         0.9900, 0.9970, 0.9983, 0.9991, 0.9995, 0.9995, 0.9997, 0.9999, 0.9999,\n",
      "         0.9999, 1.0000, 1.0000],\n",
      "        [0.0704, 0.0997, 0.1518, 0.2083, 0.2396, 0.2609, 0.2864, 0.3804, 0.4439,\n",
      "         0.5036, 0.7136, 0.7168, 0.7440, 0.7746, 0.8251, 0.9173, 0.9810, 0.9892,\n",
      "         0.9906, 0.9974, 0.9990, 0.9997, 0.9998, 0.9999, 0.9999, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "minibatch = replay_buffer.minibatch()\n",
    "print(minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do updates of the network based on warm start episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the minibatch. \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReplayBuffer' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NN_UPDATES_PER_WARM_START):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Sample a batch from the replay buffer proportionally to the probability of sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompute the minibatch.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     minibatch \u001b[38;5;241m=\u001b[39m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Use batch to train an agent. Keep track of temporal difference errors during training.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain the agent using the minibatch.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReplayBuffer' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "for _ in range(NN_UPDATES_PER_WARM_START):\n",
    "    # Sample a batch from the replay buffer proportionally to the probability of sampling.\n",
    "    print(\"Compute the minibatch.\", \"\\n\")\n",
    "    minibatch = replay_buffer.sample()\n",
    "\n",
    "    # Use batch to train an agent. Keep track of temporal difference errors during training.\n",
    "    print(\"Train the agent using the minibatch.\", \"\\n\")\n",
    "    td_error = agent.train(minibatch)\n",
    "\n",
    "    # Update probabilities of sampling each datapoint proportionally to the error.\n",
    "    print(\"Update errors.\")\n",
    "    replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEOS KWDIKAS!!!!!! https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run multiple training iterations. Each iteration consists of:\n",
    "- generating episodes following agent's actions with exploration,\n",
    "- validation and test episodes for evaluating performance,\n",
    "- Q-network updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_episode_rewards = []\n",
    "i_episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_training = True\n",
    "for iteration in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    print(\"ITERATION {}.\".format(iteration+1))\n",
    "    # GENERATE NEW EPISODES\n",
    "    # Compute epsilon value according to the schedule.\n",
    "    epsilon = max(EPSILON_END, EPSILON_START-iteration*(EPSILON_START-EPSILON_END)/EPSILON_STEPS)\n",
    "\n",
    "    # Simulate training episodes.\n",
    "    episode_number = 1\n",
    "    \n",
    "    episode_duration = 6\n",
    "\n",
    "    for _ in range(TRAINING_EPISODES_PER_ITERATION):\n",
    "\n",
    "        # Reset the environment to start a new episode.\n",
    "        classifier_state, _action, reward = env.reset()\n",
    "        batch = 6\n",
    "        done = False\n",
    "        episode_duration = 6\n",
    "        done = False\n",
    "\n",
    "        # Keep track of stats of episode to analyse it in tensorboard.\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Run an episode.\n",
    "        while not done:\n",
    "            train_batch = batch\n",
    "            if (train_batch-2 >= env.n_actions) or (train_batch+2 >= env.n_actions) or (train_batch >= env.n_actions):\n",
    "                done = True\n",
    "            else:\n",
    "                train_batch = env._find_batch_size(train_batch, reward, env.n_actions)\n",
    "                episode_duration += train_batch\n",
    "                action = agent.get_action(classifier_state, _action, train_batch)\n",
    "\n",
    "                if np.random.ranf() < epsilon:\n",
    "                    inputNumbers =range(0,env.n_actions)\n",
    "                    action = np.array(random.sample(inputNumbers, train_batch))\n",
    "\n",
    "                # With epsilon probability, take a random action.\n",
    "                # action is a vector that corresponds to a taken action.\n",
    "                action = _action[:,action]\n",
    "\n",
    "                # Make another step.\n",
    "                _state, _action, reward, done = env.step(action)\n",
    "\n",
    "                # Store a step in replay buffer.\n",
    "                replay_buffer.store_transition(classifier_state, \n",
    "                                            action, \n",
    "                                            reward, \n",
    "                                            _state, \n",
    "                                            _action, \n",
    "                                            done)\n",
    "                \n",
    "                # Change a state of environment.\n",
    "                classifier_state = _state\n",
    "        print(\"Iteration {}. | Episode {}. | Budget size is {}.\".format(iteration, episode_number, episode_duration))\n",
    "        episode_number += 1\n",
    "\n",
    "    # VALIDATION AND TEST EPISODES.\n",
    "    episode_summary = tf.Summary()\n",
    "    if iteration%VALIDATION_TEST_FREQUENCY == 0:\n",
    "\n",
    "        # Validation episodes are run. Use env for it.\n",
    "        print(\"\\n\\n\\n\")\n",
    "        print(\"Iteration {}. Validation episodes.\".format(iteration+1))\n",
    "        all_durations_validation = []\n",
    "        for i in range(N_VALIDATION):\n",
    "            done = False\n",
    "            state, _action, reward = env.reset()\n",
    "            validation_batch = batch\n",
    "            all_durations_validation.append(validation_batch)\n",
    "            while not(done):\n",
    "                if (validation_batch-2 >= env.n_actions) or (validation_batch+2 >= env.n_actions) or (validation_batch >= env.n_actions):\n",
    "                    done = True\n",
    "                else:\n",
    "                    validation_batch = env._find_batch_size(validation_batch, reward, env.n_actions)\n",
    "                    all_durations_validation.append(validation_batch)\n",
    "                    action = policy_rl(agent, state, _action, validation_batch)        \n",
    "                    action = _action[:,action]\n",
    "                    next_state, _action, reward, done = env.step(action)\n",
    "                    state = next_state\n",
    "        \n",
    "        # Test episodes are run. Use env_test for it.\n",
    "        print(\"\\n\\n\\n\")\n",
    "        print(\"Iteration {}. Test episodes.\".format(iteration+1))\n",
    "        all_durations_test = []\n",
    "        for i in range(N_TEST):\n",
    "            done = False\n",
    "            state, _action, reward = env_test.reset()\n",
    "            test_batch = batch\n",
    "            all_durations_test.append(test_batch)\n",
    "            while not(done):\n",
    "                if (test_batch-2 >= env_test.n_actions) or (test_batch+2 >= env_test.n_actions) or (test_batch >= env_test.n_actions):\n",
    "                    done = True\n",
    "                else:\n",
    "                    test_batch = env_test._find_batch_size(test_batch, reward, env_test.n_actions)\n",
    "                    all_durations_test.append(test_batch)\n",
    "                    action = policy_rl(agent, state, _action, test_batch)        \n",
    "                    action = _action[:,action]\n",
    "                    next_state, _action, reward, done = env_test.step(action)\n",
    "                    state = next_state\n",
    "   \n",
    "    # NEURAL NETWORK UPDATES.\n",
    "    for _ in range(NN_UPDATES_PER_ITERATION):\n",
    "        minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "        td_error = agent.train(minibatch)\n",
    "        replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.episode_qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### To see the results in tensorboard:\n",
    "\n",
    "On the server:\n",
    "tensorboard --logdir=./\n",
    "\n",
    "Or:\n",
    "tensorboard --logdir ./  --port 6006  --host 0.0.0.0\n",
    "\n",
    "On the computer:\n",
    "ssh -N -f -L localhost:6006:localhost:6006 konyushk@iccvlabsrv20.iccluster.epfl.ch && open http://localhost:6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
